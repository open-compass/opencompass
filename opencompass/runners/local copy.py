import os
import os.path as osp
import re
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from threading import Lock
from typing import Any, Dict, List, Tuple

import mmengine
import numpy as np
from mmengine.config import ConfigDict
from mmengine.device import is_npu_available
from tqdm import tqdm

from opencompass.registry import RUNNERS, TASKS
from opencompass.utils import get_logger, model_abbr_from_cfg

from .base import BaseRunner


def get_command_template(gpu_ids: List[int]) -> str:
    """Format command template given available gpu ids."""
    if is_npu_available():
        tmpl = 'LOCAL_RANK=0 ASCEND_RT_VISIBLE_DEVICES=' + ','.join(
            str(i) for i in gpu_ids)
        tmpl += ' {task_cmd}'
    elif sys.platform == 'win32':  # Always return win32 for Windows
        # use command in Windows format
        tmpl = 'set LOCAL_RANK=0 & set CUDA_VISIBLE_DEVICES=' + ','.join(
            str(i) for i in gpu_ids)
        tmpl += ' & {task_cmd}'
    else:
        tmpl = 'LOCAL_RANK=0 CUDA_VISIBLE_DEVICES=' + ','.join(
            str(i) for i in gpu_ids)
        tmpl += ' {task_cmd}'
    return tmpl


@RUNNERS.register_module()
class LocalRunner(BaseRunner):
    """Local runner that supports multi-machine execution using torchrun.

    This runner assumes you are using torchrun to launch the script across
    multiple machines, and the distributed environment variables are set by torchrun.

    Args:
        task (ConfigDict): Task type config.
        max_num_workers (int): Max number of workers to run in parallel per GPU.
        max_workers_per_gpu (int): Max number of workers to run for one GPU.
        debug (bool): Whether to run in debug mode.
        lark_bot_url (str): Lark bot url.
        keep_tmp_file (bool): Whether to keep temporary files.
    """

    def __init__(self,
                 task: ConfigDict,
                 max_num_workers: int = 16,
                 debug: bool = False,
                 max_workers_per_gpu: int = 1,
                 lark_bot_url: str = None,
                 keep_tmp_file: bool = False,
                 **kwargs):
        super().__init__(task=task, debug=debug, lark_bot_url=lark_bot_url)
        self.max_num_workers = max_num_workers
        self.max_workers_per_gpu = max_workers_per_gpu
        self.keep_tmp_file = keep_tmp_file
        self.logger = get_logger()

        # 获取分布式环境变量
        self.rank = int(os.getenv('RANK', '0'))
        self.world_size = int(os.getenv('WORLD_SIZE', '1'))
        self.local_rank = int(os.getenv('LOCAL_RANK', '0'))
        self.local_world_size = int(os.getenv('LOCAL_WORLD_SIZE', '1'))

        # 是否分布式环境
        self.is_distributed = self.world_size > 1

        if self.is_distributed:
            self.logger.info(
                f'Running in distributed mode: '
                f'rank={self.rank+1}/{self.world_size}, '
                f'local_rank={self.local_rank+1}/{self.local_world_size}')

        # 获取节点ID（每台机器一个节点ID）
        if self.is_distributed:
            self.node_rank = self.rank // self.local_world_size
            self.num_nodes = (self.world_size + self.local_world_size -
                              1) // self.local_world_size
            self.logger.info(
                f'Node info: node_rank={self.node_rank}/{self.num_nodes}')
        else:
            self.node_rank = 0
            self.num_nodes = 1

        for k, v in kwargs.items():
            self.logger.warning(
                f'Ignored argument in {self.__module__}: {k}={v}')

    def launch(self, tasks: List[Dict[str, Any]]) -> List[Tuple[str, int]]:
        """Launch multiple tasks.

        Args:
            tasks (list[dict]): A list of task configs, usually generated by
                Partitioner.

        Returns:
            list[tuple[str, int]]: A list of (task name, exit code).
        """
        # 预分配任务到不同节点（每台机器）
        if self.is_distributed:
            # 首先按节点分配任务
            tasks_per_node = len(tasks) // self.num_nodes
            remainder = len(tasks) % self.num_nodes

            node_start_idx = self.node_rank * tasks_per_node
            if self.node_rank < remainder:
                node_start_idx += self.node_rank
                tasks_per_node += 1
            else:
                node_start_idx += remainder

            node_end_idx = node_start_idx + tasks_per_node
            node_tasks = tasks[node_start_idx:node_end_idx]

            # 然后在节点内按local_rank再次分配
            if self.local_world_size > 1:
                tasks_per_local_rank = len(node_tasks) // self.local_world_size
                local_remainder = len(node_tasks) % self.local_world_size

                local_start_idx = self.local_rank * tasks_per_local_rank
                if self.local_rank < local_remainder:
                    local_start_idx += self.local_rank
                    tasks_per_local_rank += 1
                else:
                    local_start_idx += local_remainder

                local_end_idx = local_start_idx + tasks_per_local_rank
                rank_tasks = node_tasks[local_start_idx:local_end_idx]
            else:
                rank_tasks = node_tasks

            self.logger.info(
                f'Node {self.node_rank}, Rank {self.rank}: Processing {len(rank_tasks)} tasks'
            )
        else:
            rank_tasks = tasks

        status = []

        # 获取可用GPU资源
        import torch
        if is_npu_available():
            visible_devices = 'ASCEND_RT_VISIBLE_DEVICES'
            device_nums = torch.npu.device_count()
        else:
            visible_devices = 'CUDA_VISIBLE_DEVICES'
            device_nums = torch.cuda.device_count()

        if visible_devices in os.environ:
            all_gpu_ids = [
                int(i)
                for i in re.findall(r'(?<!-)\d+', os.getenv(visible_devices))
            ]
        else:
            all_gpu_ids = list(range(device_nums))

        # 根据local_rank限制使用的GPU，避免进程间争抢同一GPU
        if self.is_distributed and self.local_world_size > 1:
            # 计算每个进程可以使用的GPU数量
            gpus_per_proc = len(all_gpu_ids) // self.local_world_size
            if gpus_per_proc == 0:
                # 如果GPU数量少于进程数，则每个进程使用一个GPU
                if self.local_rank < len(all_gpu_ids):
                    all_gpu_ids = [all_gpu_ids[self.local_rank]]
                else:
                    # 如果没有足够的GPU，使用默认的GPU 0
                    all_gpu_ids = [0]
                    self.logger.warning(
                        f'Not enough GPUs for all processes. Rank {self.rank} will use GPU 0.'
                    )
            else:
                # 按local_rank分配GPU
                start_idx = self.local_rank * gpus_per_proc
                end_idx = start_idx + gpus_per_proc
                all_gpu_ids = all_gpu_ids[start_idx:end_idx]

        self.logger.info(f'Rank {self.rank} will use GPUs: {all_gpu_ids}')

        if len(all_gpu_ids) > 0:
            gpus = np.zeros(max(all_gpu_ids) + 1, dtype=np.uint)
            gpus[all_gpu_ids] = self.max_workers_per_gpu
        else:
            gpus = np.array([], dtype=np.uint)

        if self.debug:
            # Debug mode - run tasks sequentially
            for task in rank_tasks:
                task = TASKS.build(dict(cfg=task, type=self.task_cfg['type']))
                task_name = task.name
                num_gpus = task.num_gpus
                assert len(
                    all_gpu_ids
                ) >= num_gpus, f'Not enough GPUs: need {num_gpus}, have {len(all_gpu_ids)}'

                # Create parameter file
                mmengine.mkdir_or_exist('tmp/')
                import uuid
                uuid_str = str(uuid.uuid4())
                param_file = f'tmp/{uuid_str}_params.py'

                try:
                    task.cfg.dump(param_file)
                    if len(all_gpu_ids) > num_gpus and num_gpus > 0:
                        self.logger.warning(f'Only use {num_gpus} GPUs for '
                                            f'total {len(all_gpu_ids)} '
                                            'available GPUs in debug mode.')
                    tmpl = get_command_template(all_gpu_ids[:num_gpus])
                    cmd = task.get_command(cfg_path=param_file, template=tmpl)

                    # Execute the task
                    if 'python3 ' in cmd or 'python ' in cmd:
                        # If it is an infer type task do not reload if
                        # the current model has already been loaded.
                        if 'infer' in self.task_cfg.type.lower():
                            # If a model instance already exists,
                            # do not reload it.
                            task.run(cur_model=getattr(self, 'cur_model',
                                                       None),
                                     cur_model_abbr=getattr(
                                         self, 'cur_model_abbr', None))
                            self.cur_model = task.model
                            self.cur_model_abbr = model_abbr_from_cfg(
                                task.model_cfg)
                        else:
                            task.run()
                    else:
                        tmp_logs = f'tmp/{os.getpid()}_debug.log'
                        self.logger.warning(
                            f'Debug mode, log will be saved to {tmp_logs}')
                        with open(tmp_logs, 'a') as log_file:
                            subprocess.run(cmd,
                                           shell=True,
                                           text=True,
                                           stdout=log_file,
                                           stderr=subprocess.STDOUT)
                finally:
                    if not self.keep_tmp_file:
                        os.remove(param_file)
                status.append((task_name, 0))
        else:
            # Normal mode - run tasks in parallel
            pbar = tqdm(total=len(rank_tasks))
            lock = Lock()

            def submit(task, index):
                """提交单个任务的函数，用于并行或顺序执行."""
                task = TASKS.build(dict(cfg=task, type=self.task_cfg['type']))
                num_gpus = task.num_gpus

                if num_gpus > len(all_gpu_ids):
                    self.logger.warning(
                        f'Task requires {num_gpus} GPUs but only {len(all_gpu_ids)} available. Using all available GPUs.'
                    )
                    num_gpus = len(all_gpu_ids)

                # 获取GPU资源
                while True:
                    lock.acquire()
                    if sum(gpus > 0) >= num_gpus:
                        gpu_ids = np.where(gpus)[0][:num_gpus]
                        gpus[gpu_ids] -= 1
                        lock.release()
                        break
                    lock.release()
                    time.sleep(1)

                if num_gpus > 0:
                    tqdm.write(
                        f'Node {self.node_rank}, Rank {self.rank}: Launch {task.name} on GPU '
                        + ','.join(map(str, gpu_ids)))
                else:
                    tqdm.write(
                        f'Node {self.node_rank}, Rank {self.rank}: Launch {task.name} on CPU'
                    )

                # 执行任务
                res = self._launch(task, gpu_ids, index)
                pbar.update()

                # 释放GPU资源
                with lock:
                    gpus[gpu_ids] += 1

                return res

            # 顺序执行当前rank分配到的任务
            self.logger.info(
                f'Node {self.node_rank}, Rank {self.rank}: Starting sequential execution of {len(rank_tasks)} tasks'
            )
            for i, task in enumerate(rank_tasks):
                task_result = submit(task, i)
                status.append(task_result)
            self.logger.info(
                f'Node {self.node_rank}, Rank {self.rank}: Completed all tasks'
            )

        return status

    def _launch(self, task, gpu_ids, index):
        """Launch a single task.

        Args:
            task (BaseTask): Task to launch.
            gpu_ids (List[int]): GPU IDs to use
            index (int): Task index

        Returns:
            tuple[str, int]: Task name and exit code.
        """
        task_name = task.name

        # Create parameter file
        pwd = os.getcwd()
        mmengine.mkdir_or_exist('tmp/')
        import uuid
        uuid_str = str(uuid.uuid4())
        param_file = f'{pwd}/tmp/{uuid_str}_params.py'

        try:
            task.cfg.dump(param_file)
            tmpl = get_command_template(gpu_ids)
            get_cmd = partial(task.get_command,
                              cfg_path=param_file,
                              template=tmpl)
            cmd = get_cmd()

            self.logger.debug(f'Running command: {cmd}')

            # Run command
            out_path = task.get_log_path(file_extension='out')
            mmengine.mkdir_or_exist(osp.split(out_path)[0])
            stdout = open(out_path, 'w', encoding='utf-8')

            result = subprocess.run(cmd,
                                    shell=True,
                                    text=True,
                                    stdout=stdout,
                                    stderr=stdout)

            if result.returncode != 0:
                self.logger.error(f'Task {task_name} fail, see\n{out_path}')
        finally:
            # Clean up
            if not self.keep_tmp_file:
                os.remove(param_file)

        return task_name, result.returncode
